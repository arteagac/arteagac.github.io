{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1dda89f-f98a-41f7-bd60-61ad448c2cab",
   "metadata": {},
   "source": [
    "# Run BLOOM, the largest open-source AI model, on your desktop computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30bd79c-4e4d-4be7-bc4a-17cc3b6c6aa2",
   "metadata": {},
   "source": [
    "See BLOOM in action solving math, translation, and coding problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842fe323-f7cc-458e-b9ba-e1b897a8c01b",
   "metadata": {},
   "source": [
    "By: Cristian Arteaga, [arteagac.github.io](https://arteagac.github.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea69d8f-f8c1-42d2-9346-2924eb47c083",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bfb7d4-f2e8-4b05-aac8-eea92a37696b",
   "metadata": {},
   "source": [
    "BLOOM is an open-source language model that contains 176 billion parameters and was trained for 3.5 months on 384 A100-80GB GPUs. A BLOOM checkpoint takes 330 GB of disk storage, so it seems unfeasible to run this model on a desktop computer. However, this is a feasible task if you have enough disk space, at least 16GB of RAM, and some patience (you don't even need a GPU). \n",
    "\n",
    "BLOOM is a collaborative effort of more than 1,000 scientist and the amazing HuggingFace team. It is remarkable that such large multi-lingual model is openly available for everybody. By the end of this tutorial, you will learn how to run this massive language model your local computer and see it in action generating texts such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092f1dc-a8b2-4098-9276-6d6df142dc11",
   "metadata": {},
   "source": [
    "```\n",
    "-  INPUT: \"The SQL command to extract all the users whose name starts with A is: \"\n",
    "   OUTPUT: \"SELECT * FROM users WHERE name LIKE 'A%'\"\n",
    "\n",
    "-  INPUT: \"The Spanish translation of thank you for your help is:\"\n",
    "   OUTPUT: \"gracias por su ayuda\n",
    "\n",
    "-  INPUT: \"John is 4 times as old as Bob. Bob is 3 years younger than Mike. Mike is 10 years old. What is John's age? Let's think step by step\".\n",
    "  OUTPUT: \"First, we need to find out how old Bob is. Bob is 3 years younger than Mike. So, Bob is 10-3=7 years old. Now, we need to find out how old John is. John is 4 times as old as Bob. So, John is 4 times 7=28 years old\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e2690-ac74-4402-8958-0f7a5d86508b",
   "metadata": {},
   "source": [
    "This tutorial uses some components of the HuggingFace's transformers library, along with custom Python code to strategically load the model weights from disk and generate a sequence of tokens. For the sake of learning, the inference Python code in this tutorial was written from scratch and does not use the out-of-the-box implementation available in HuggingFace Accelerate. For production, HuggingFace Accelerate is much more robust and versatile. The Python code in this tutorial generates one token every 3 minutes on a computer with an i5 11gen processor, 16GB of RAM, and a Samsung 980 PRO NVME hard drive (a fast hard drive can significantly increase inference speeds)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272326c4-015a-457c-83d0-39cb470efeb8",
   "metadata": {},
   "source": [
    "## BLOOM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dec2cff-6944-405e-8bbf-ccf8f8c800a4",
   "metadata": {},
   "source": [
    "BLOOM is a causal model language, which means that it was trained as a next-token predictor. This apparently simple strategy of predicting the next token in a sentence, based on a set of preceding tokens, has shown to capture certain degree of reasoning abilities for large language models (arXiv:2205.11916). This enables BLOOM and similar models to connect multiple concepts in a sentence and manage to solve non-trivial problems such as arithmetic, translation, and programming with fair accuracy. BLOOM uses a Transformer architecture composed of an input embeddings layer, 70 Transformer blocks, and an output language-modeling layer, as shown in the figure below. Each Transformer block has a self-attention layer and a multi-layer perceptron layer, with input and post-attention layer norms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6885b6d-aba8-4973-baf1-def335050c4a",
   "metadata": {},
   "source": [
    "<img width=600 src=\"bloom_local/bloom_architecture.jpg\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab5a66-5c98-4b70-8bda-6185f7f6f641",
   "metadata": {},
   "source": [
    "To predict the next token in a sentence using BLOOM, we simply need to sequentially pass the input  tokens (in the form of embeddings) through each of 70 BLOOM blocks. Given that this is a sequential operation, we can load into RAM only one block at a time to avoid memory overflow. Similarly, the word embeddings and output language-modeling layer can be loaded on-demand from disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa227c-4b8f-4254-b397-24792a75428e",
   "metadata": {},
   "source": [
    "## Pre-trained BLOOM checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e4bde0-30f1-4ef8-9fbc-ff9349ac56b1",
   "metadata": {},
   "source": [
    "The code below downloads the BLOOM (176-B version) from the HuggingFace models repository: https://huggingface.co/bigscience/bloom. This downloads only a specific BLOOM checkpoint without any additional git history or linked files. Make sure you have enough disk space (around 330 GB).\n",
    "\n",
    "``` bash\n",
    "git lfs install\n",
    "export GIT_LFS_SKIP_SMUDGE=1\n",
    "git clone https://huggingface.co/bigscience/bloom\n",
    "cd bloom\n",
    "git lfs fetch origin 2a3d62e\n",
    "git lfs checkout\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baae56f3-c407-48a4-86d8-5b0fd1a37506",
   "metadata": {},
   "source": [
    "The downloaded folder contains a sharded BLOOM checkpoint, as shown below. Sharded means that the checkpoint was split into 72 different files named \"pytorch_model_00001-of-00072.bin\" to \"pytorch_model_00001-of-00072.bin\" for convenient handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588fffe-7a0d-4624-97d6-663b68d3a653",
   "metadata": {},
   "source": [
    "```\n",
    "> ls -la\n",
    "6.7 GB  pytorch_model_00001-of-00072.bin \n",
    "4.6 GB  pytorch_model_00002-of-00072.bin \n",
    "...\n",
    "4.6 GB  pytorch_model_00071-of-00072.bin\n",
    "4.6 GB  pytorch_model_00072-of-00072.bin\n",
    "0.5 KB  config.json\n",
    " 14 MB  tokenizer.json\n",
    " 13 KB  pytorch_model.bin.index.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6b1bc0-f8b3-44c1-a4cc-e81b5d7a4ff0",
   "metadata": {},
   "source": [
    "The file `00001` contains the word embeddings and associated layer norm, the files `00002` to `00071` contain the 70 BLOOM blocks, and the file `00072` contains the final layernorm. The output language modeling layer uses the same weights as the word embeddings. In case you are curious, the `pytorch_model.bin.index.json` file specifies how the BLOOM layers are distributed across the shards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ec95db-ba04-49ce-b337-8fa07713c43a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6db2f4-55b3-4651-9656-73ecdbcc17b1",
   "metadata": {},
   "source": [
    "Now let's use the downloaded BLOOM model to do inference. First, we need to install HuggingFace `transformers` v4.20.0, as shown below. This specific version is required, as the custom Python code in this tutorial uses methods available only in this specific version of `transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c22f14-4ac9-403f-a899-da121cff1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c40b316-77e8-4d7f-bd09-e7f0de1e6913",
   "metadata": {},
   "source": [
    "Second, we create a method (`get_state_dict`) that takes as input a shard number (1 to 72), reads the shard from disk, and returns a dictionary with the model state. This method allows to remove prefixes from the dictionary keys to facilitate loading the weights into the model objects using `torch.load_state_dict`. We also create the tokenizer and configuration objects by loading them from the downloaded folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aa55af-e5cf-4d89-87d3-6a232a1af866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "def get_state_dict(shard_num, prefix=None):\n",
    "    d = torch.load(os.path.join(model_path, f\"pytorch_model_{shard_num:05d}-of-00072.bin\"))\n",
    "    return d if prefix is None else OrderedDict((k.replace(prefix, ''), v) for k, v in d.items())\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BloomConfig\n",
    "from transformers.models.bloom.modeling_bloom import BloomBlock, build_alibi_tensor\n",
    "\n",
    "model_path = \"/home/das/nvme/bloom\" # replace with your local folder path\n",
    "config = BloomConfig.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e66db-ee2e-4b2a-8d00-690f5b651063",
   "metadata": {},
   "source": [
    "Third, we create three methods to load the state dictionaries into different model objects. We use these methods during inference to load only specific parts of the model to RAM. These three methods follow a similar pattern that consists of 1) reading a shard from disk, 2) creating a model object, 3) filling up the weights of the model object using `torch.load_state_dict`, and 4) returning the model object. The only exception is the `load_block` method, which does not create a new block object but instead overwrites an object passed as parameter to save RAM memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17048d06-9e01-4ee3-8c49-737009e9a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "    state_dict = get_state_dict(shard_num=1, prefix=\"word_embeddings_layernorm.\")\n",
    "    embeddings = nn.Embedding.from_pretrained(state_dict.pop('word_embeddings.weight'))\n",
    "    lnorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon, dtype=torch.bfloat16)\n",
    "    lnorm.load_state_dict(state_dict)\n",
    "    return embeddings.to(device), lnorm.to(device)\n",
    "\n",
    "def load_causal_lm_head():\n",
    "    linear = nn.utils.skip_init(\n",
    "        nn.Linear, config.hidden_size, config.vocab_size, bias=False, dtype=torch.bfloat16)\n",
    "    linear.load_state_dict(get_state_dict(shard_num=1, prefix=\"word_embeddings.\"), strict=False)\n",
    "    return linear.bfloat16().to(device)\n",
    "\n",
    "def load_block(block_obj, block_num):\n",
    "    block_obj.load_state_dict(get_state_dict(shard_num=block_num + 2, prefix=f\"h.{block_num}.\"))\n",
    "    block_obj.to(device)\n",
    "\n",
    "final_lnorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon, dtype=torch.bfloat16)\n",
    "final_lnorm.load_state_dict(get_state_dict(shard_num=72, prefix=\"ln_f.\"))\n",
    "final_lnorm.to(device)\n",
    "block = BloomBlock(config, layer_number=1).bfloat16()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f6366-54b8-4dfa-8509-9359707696a5",
   "metadata": {},
   "source": [
    "Fourth, we create a method to do a full forward pass through all the BLOOM's layers. This method takes as input an array of token input ids, and returns the token id predicted as next in the sentence. The method starts by creating an attention mask and the position encodings (alibi). Then, it does a forward pass on the embedding layer to create the initial `hidden_states`. Next, it sequentially passes the `hidden_states` through the 70 BLOOM blocks and the output language model head to generate the output logits. The `argmax` takes the output logits and returns the token id with highest prediction probability. Note that, after using the embeddings, we delete them to avoid overflowing the memory. Also, every time we call a bloom block, we read the new object and overwrite the weights of the existing `block` object to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f24b4e-7f34-4c27-ab2b-a9f7ae23ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(input_ids):\n",
    "    # 1. Create attention mask and position encodings\n",
    "    attention_mask = torch.ones(len(input_ids)).unsqueeze(0).bfloat16().to(device)\n",
    "    alibi = build_alibi_tensor(input_ids.shape[1], config.num_attention_heads,\n",
    "                               torch.bfloat16).to(device)\n",
    "    # 2. Load and use word embeddings\n",
    "    embeddings, lnorm = load_embeddings()\n",
    "    hidden_states = lnorm(embeddings(input_ids))\n",
    "    del embeddings, lnorm\n",
    "\n",
    "    # 3. Load and use the BLOOM blocks sequentially\n",
    "    for block_num in range(70):\n",
    "        load_block(block, block_num)\n",
    "        hidden_states = block(hidden_states, attention_mask=attention_mask, alibi=alibi)[0]\n",
    "        print(\".\", end='')\n",
    "    \n",
    "    hidden_states = final_lnorm(hidden_states)\n",
    "    \n",
    "    #4. Load and use language model head\n",
    "    lm_head = load_causal_lm_head()\n",
    "    logits = lm_head(hidden_states)\n",
    "\n",
    "    # 5. Compute next token \n",
    "    return torch.argmax(logits[:, -1, :], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacb31f7-ce01-45e3-8c35-62b4b0749d0e",
   "metadata": {},
   "source": [
    "Finally, we sequentially call the forward method to predict the next tokens in the sentence, one token at a time. Note that, at every step, we concatenate the newly generated token with the previous tokens (`input_ids`) to further generate additional tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d5c60-23ef-4050-8b8d-0ee7f795c812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_sentence = \"The SQL command to extract all the users whose name starts with A is: \"\n",
    "input_ids = tokenizer.encode(input_sentence, return_tensors='pt').to(device)\n",
    "max_tokens = 10\n",
    "for i in range(max_tokens): \n",
    "    print(f\"Token {i + 1} \", end='')\n",
    "    new_id = forward(input_ids)\n",
    "    input_ids = torch.cat([input_ids, new_id.unsqueeze(-1)], dim=-1)\n",
    "    print(tokenizer.decode(new_id))\n",
    "\n",
    "print(tokenizer.decode(input_ids.squeeze(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ebf69-3814-4bbd-91ef-c1449ddc50e7",
   "metadata": {},
   "source": [
    "```\n",
    "OUTPUT:\n",
    "Token 1 ...................................................................... SELECT\n",
    "Token 2 ...................................................................... *\n",
    "Token 3 ...................................................................... FROM\n",
    "Token 4 ...................................................................... users\n",
    "Token 5 ...................................................................... WHERE\n",
    "Token 6 ...................................................................... name\n",
    "Token 7 ...................................................................... LIKE\n",
    "Token 8 ...................................................................... 'A\n",
    "Token 9 ......................................................................%'\n",
    "Token 10 ......................................................................\n",
    "\n",
    "The SQL command to extract all the users whose name starts with A is:  SELECT * FROM users WHERE name LIKE 'A%'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b620a034-0dc6-4253-944e-6f983b54de6b",
   "metadata": {},
   "source": [
    "Every dot in the output represents a forward pass on each of the BLOOM blocks. This output shows that BLOOM can effectively generate a SQL sentence. You can run other examples (for instance, the one mentioned at the beginning of this tutorial) to see how powerful BLOOM is. Just remember to increase the number of tokens to generate using the `max_tokens` variable. A Jupyter Notebook with all the source code in this tutorial is available in the Blog section of my website: https://arteagac.github.io/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_kernel",
   "language": "python",
   "name": "hf_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
